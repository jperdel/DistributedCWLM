{"cells":[{"cell_type":"code","source":["# TODO:\n\n# - M-step sin loops\n# - Adjust batch size in the GD for the weights\n# - sample VS takeSample (rdd, probabilistico VS lista, tamaño fijo)\n\n# - Investigar umbral para las responsibilities\n# - Eficiencia de withColumn vs select\n# - Eficiencia RDD vs DF para recalcular parámetros\n# - Producto matricial en M-step para D > 1\n# - Lista de componentes VS arrays (comp[i].w VS x[i,:,:])\n# - Pesos con intercepto o intercepto a parte (w * x_ext VS w0 + w * x)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21b1bd18-a22a-401f-a50e-8f3fb1a26fcd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import numpy as np\nfrom scipy.stats import norm, multivariate_normal\nfrom scipy.special import logsumexp # previously from scipy.misc import logsumexp\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nsb.set()\n\nfrom pyspark.ml.clustering import GaussianMixture\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import DenseVector, SparseVector, Vectors, VectorUDT\nfrom pyspark.sql.functions import udf, mean as _mean\nfrom pyspark.sql.types import *\n\n#Clase componente para facilitar la manipulación del modelo\nclass Component:\n\n    def __init__(self, pi, mu, sigma, w, betaInv):\n        self.pi = pi\n        self.mu = mu\n        self.sigma = sigma\n        self.w = w\n        self.betaInv = betaInv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"181f429e-1553-438d-a6c9-cad21781265f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Block with the util functions (utils.py)\n\ndef _computeResp(features, target, components):\n      \n        logPdfCompList = list()\n\n        for component in components:\n          \n            pi = component.pi\n            mu = component.mu\n            sigma = component.sigma\n            w = component.w\n            betaInv = component.betaInv\n\n            logPdfComp = (np.log(pi) + \n                          multivariate_normal.logpdf(features[1:], mean=mu, cov=sigma) + \n                          norm.logpdf(target, loc=np.dot(w,features), scale=np.sqrt(betaInv)))\n\n            logPdfCompList.append(logPdfComp)\n\n        logPdfComp = np.array(logPdfCompList)\n\n        logPdf = logsumexp(logPdfComp)\n        resp = np.exp(logPdfComp - logPdf)\n\n        # TODO: probar si este bloque afecta. Normalizar L1??\n        resp = resp * (resp > 1e-10)\n        resp = resp / sum(resp)\n        \n        results = np.insert(resp, 0, logPdf)\n\n        return Vectors.dense(results)\n      \n# UDFs block\npdfUdf = udf(lambda x: float(x[0]),  FloatType())\nrespUdf = udf(lambda x: Vectors.dense(x[1:]),  VectorUDT())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbfed533-c1ea-4f26-8eb3-f69513b1404e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class ClusterwiseLinearModel:\n  \n    def __init__(self, K):\n      \n        self.path2data = None # With a normal dataset, change this\n        self.components = None\n        self.dataTr = None\n        self.dataVal = None\n        self.dataTst = None\n        self.dim = None\n        self.K = K\n        self.logLikelihoodEvol = list()\n        \n        return\n      \n    def loadData(self):\n      \n        # In this case, no file with the data. We would have something like:\n        #data = loadmat(path2data)['data_mat']\n\n        # For this case with an artificial dataset\n        nPoints = 100\n        \n        betaInvTrue = 1\n        X = np.random.uniform(-10,10,nPoints)[:,np.newaxis]\n        Y = (10 / (1 + np.exp(X)) + np.random.normal(0,np.sqrt(betaInvTrue),(nPoints,1)))\n        # A partir de aquí, compartido ##########################################################\n        \n        # Añadimos los IDs\n        ids = np.arange(nPoints)[:,np.newaxis]\n\n        # Paralelización de los datos (sólo train)\n        data = np.hstack([ids,X,Y]).astype(float)\n        dataRDD = sc.parallelize(data)\n        \n        # Get the dimensionality of the dataset\n        self.dim = X.shape[1]\n\n        dataDF = (dataRDD.map(lambda x: x.tolist()).toDF()\n                         .withColumnRenamed('_1', 'ID')\n                         .withColumnRenamed('_'+str(self.dim+2), 'target'))\n        \n        # Get all the columns to generate a unique feature column\n        col_names = list(range(2, self.dim+2))\n        col_names = ['_' + str(col) for col in col_names]\n\n        # Generate the features column\n        vectorAssembler = VectorAssembler(inputCols = col_names, outputCol = 'features')\n        dataDF = vectorAssembler.transform(dataDF)\n        dataDF = dataDF.select(['ID', 'features', 'target'])\n        \n        # Train-Val-Test split\n        #[dataTrDF, dataValDF, dataTstDF] = dataDF.randomSplit([0.98,0.01,0.01])\n\n        self.dataTr = dataDF\n        self.dataVal = None\n        self.dataTst = None\n        \n        return\n      \n    def initModel(self):\n      \n        # Get the data\n        dataDF = self.dataTr\n        \n        # Cluster the data\n        gmm = (GaussianMixture().setK(self.K)\n                                .setFeaturesCol('features'))\n        \n        modelGMM = gmm.fit(dataDF)\n        gmmDF = modelGMM.transform(dataDF)\n        \n        # Get the parameters for the linear regression\n        lr = (LinearRegression().setMaxIter(10)\n                                .setRegParam(0.3)\n                                .setElasticNetParam(0.8)\n                                .setFeaturesCol('features')\n                                .setLabelCol('target'))\n        \n        \n        # List to save the components\n        compList = list() \n        \n        # Get the pi from the GMM\n        pi = modelGMM.weights\n        \n        for i in range(self.K):\n\n            # Filter the DF by cluster\n            dataFiltDF = gmmDF.filter(gmmDF['prediction'] == i).select('ID','features','target')\n\n            # Fit the model\n            modelLr = lr.fit(dataFiltDF)\n            \n            # Component parameters\n            mu = modelGMM.gaussians[i].mean\n            sigma = modelGMM.gaussians[i].cov.toArray()\n            #w0 = modelLr.intercept\n            w = np.hstack([np.array([modelLr.intercept]), modelLr.coefficients])\n            betaInv = modelLr.summary.objectiveHistory[-1]\n            \n            # Append component\n            compList.append(Component(pi[i], mu, sigma, w, betaInv))\n\n        self.components = compList\n        \n        # Add the ones columns for the bias\n        self.addBiasColumn()\n        \n        return\n      \n    def addBiasColumn(self):\n        \n        \"\"\"Add a ones column for simplicity of the code\"\"\"\n\n        addOnesColumnUdf = udf(lambda features: Vectors.dense(np.insert(features, 0, 1.0)), VectorUDT())\n        \n        # For train data\n        df = self.dataTr\n        df = df.withColumn('features', addOnesColumnUdf(df['features']))\n        self.dataTr = df\n\n        # For validation data\n        #df = self.dataVal\n        #df = df.withColumn('features', addOnesColumnUdf(df['features']))\n        #self.dataVal = df\n\n        # For test data\n        #df = self.dataTst\n        #df = df.withColumn('features', addOnesColumnUdf(df['features']))\n        #self.dataTst = df\n        \n        return\n        \n    def E_step(self):\n      \n        df = self.dataTr\n        components = self.components\n        \n        # We have to redefine the UDF in each iteration so the components are updated\n        computeRespUdf = udf(lambda features, target: _computeResp(features, target, components),  VectorUDT())\n        \n        df = df.withColumn('responsibilities', computeRespUdf(df['features'], df['target']))\n        \n        df = df.withColumn('logPDF', pdfUdf(df['responsibilities']))\n        df = df.withColumn('responsibilities', respUdf(df['responsibilities']))\n        \n        self.dataTr = df\n        \n        df_stats = df.select(\n            _mean(df['logPDF']).alias('loglikelihood'),\n        ).collect()\n\n        loglikelihood = df_stats[0]['loglikelihood']\n        \n        self.logLikelihoodEvol.append(loglikelihood)\n        \n        return\n      \n    def M_step(self):\n      \n        df = self.dataTr\n      \n        Nk = (df.select('responsibilities')\n                .rdd\n                .map(lambda x: np.array(x))\n                .reduce(lambda x,y: x+y)\n             )[0]\n\n\n        piNew = Nk / sum(Nk)\n        muNew = (df.select('features', 'responsibilities')\n                   .rdd\n                   .map(lambda x: x[1] * x[0][1:])\n                   .reduce(lambda x,y: x+y)\n                ) / Nk\n\n        sigmaNew = (df.select('features', 'responsibilities')\n                      .rdd\n                      .map(lambda x: np.array(x[1]) * np.square(x[0][1:] - muNew))\n                      .reduce(lambda x,y: x+y)\n                   )[0] / Nk\n        \n        wNew = list()\n\n        for i in range(mrgr.K):\n\n            counter = 1\n            gradient = np.inf\n            w = components[i].w\n            batchProp = 0.1  \n            threshold = 0.001\n            nMaxIter = 30\n            alpha = 0.0001\n\n\n            while np.linalg.norm(gradient) > threshold and counter <= nMaxIter:\n\n                batchRDD = (df.select('features', 'target', 'responsibilities')\n                              .rdd\n                              .sample(False, batchProp))\n\n                batchSize = batchRDD.count()\n\n                gradient = (batchRDD.map(lambda data: (data[2][i] * np.dot(data[0], w) - data[1]) * data[0])\n                                    .reduce(lambda x,y : x + y) \n                            / (batchSize * np.sqrt(counter)))\n\n                w = w - alpha * gradient\n\n                counter += 1\n\n            wNew.append(w)\n        \n        \n        # Update components\n        compsNew = list()\n        \n        for i in range(self.K):\n        \n            compsNew.append(Component(piNew[i], muNew[i], sigmaNew[i], wNew[i], components[i].betaInv))\n\n        self.components = compsNew\n        \n        return\n      \n    def plotComponents(self):\n      \n        color = ['r-','g-','b-','k-']\n    \n        x_axis = np.linspace(-10,10,5)\n\n        fig = plt.figure()\n        \n        x = self.dataTr.rdd.map(lambda r: r[1][1]).collect()\n        y = self.dataTr.rdd.map(lambda r: r[2]).collect()\n        \n        plt.plot(x, y, 'k.')\n\n        for i in range(self.K):\n\n            regr = self.components[i].w[0] + self.components[i].w[1] * x_axis      \n            plt.plot(x_axis,regr, color[i])\n\n        plt.xlabel('features')\n        plt.ylabel('target')    \n\n        axes = plt.gca()\n        axes.set_ylim([-3,13])\n\n        display(fig)\n        \n        return\n        \n    def plotLogLikelihood(self):\n      \n        llevol = self.logLikelihoodEvol\n        nIter = list(range(1,len(llevol)+1))\n        \n        fig = plt.figure()\n        plt.plot(nIter,llevol)\n        \n        plt.xlabel('# of iterations')\n        plt.ylabel('Log-likelihood')\n        \n        display(fig)\n        \n        return\n      \n    def printComponents(self):\n        \n        print('-------------------------------------')\n        print()\n        \n        i = 0\n        for component in self.components:\n  \n            print(f'* Component #{i}:')\n            print('  |--> pi: ', component.pi)\n            print('  |--> mu: ', component.mu)\n            print('  |--> sigma: ', component.sigma)\n            print('  |--> w: ', component.w)\n            print('  |--> betaInv: ', component.betaInv)\n            \n            i += 1\n            \n        print()\n        \n        return"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9646915b-7b52-4563-b7b6-3d6c4efa7ce5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["for i in range(10):\n\n    mrgr = ClusterwiseLinearModel(3)\n    mrgr.loadData()\n    mrgr.initModel()\n\n\n    #mrgr.plotComponents()\n\n    for i in range(4):\n\n        components = mrgr.components\n\n        mrgr.E_step()\n        mrgr.M_step()\n\n        #mrgr.plotComponents()\n\n    mrgr.plotLogLikelihood()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c3ec885-ff6a-4e0d-be63-2d2ec2c522c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["cacahuete\n\nmrgr = ClusterwiseLinearModel(3)\nmrgr.loadData()\nmrgr.initModel()\n\n\nmrgr.E_step()\n#mrgr.M_step()\nmrgr.dataTr.show(5,False)\n\nmrgr.E_step()\n#mrgr.M_step()\nmrgr.dataTr.show(5,False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fc8cc45-fe1f-467b-ac9a-62c01fd98ce4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["mrgr = ClusterwiseLinearModel(3)\nmrgr.loadData()\nmrgr.initModel()\n\n#for component in mrgr.components:\n  \n#    print('pi: ', component.pi)\n#    print('mu: ', component.mu)\n#    print('sigma: ', component.sigma)\n#    print('w: ', component.w)\n#    print('betaInv: ', component.betaInv)\n\n#mrgr.plotComps()\n\n\n#mrgr.dataTr.show(10,False)\nmrgr.printComponents()\n\ncomponents = mrgr.components # Have to make it a global variable really\nmrgr.E_step()\nmrgr.M_step()\n\n#mrgr.dataTr.show(10,False)\nmrgr.printComponents()\n\n#mrgr.plotComps()\ncomponents = mrgr.components\nmrgr.E_step()\n\n#mrgr.dataTr.show(10,False)\nmrgr.printComponents()\n#mrgr.plotLogLikelihood()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7581f5bf-4380-486d-bdd8-f88aa19abaab"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a2b96a8-6b93-4438-a2d3-27fcbf242eec"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"TFM 1D logsumexp DF","dashboards":[],"language":"python","widgets":{},"notebookOrigID":506245650029107}},"nbformat":4,"nbformat_minor":0}
